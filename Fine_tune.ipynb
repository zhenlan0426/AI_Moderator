{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3218f4e4",
   "metadata": {},
   "source": [
    "9 + full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38a592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.6: Fast Qwen3 patching. Transformers: 4.54.1. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.635 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "save_path_surfix = \"_10\"\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Disable static KV cache (major VRAM saver for FastModel)\n",
    "os.environ[\"UNSLOTH_DISABLE_STATIC_GENERATION\"] = \"1\"\n",
    "# Optional: if you still see high VRAM, disable Unsloth compile passes\n",
    "os.environ[\"UNSLOTH_COMPILE_DISABLE\"] = \"1\"\n",
    "# # for debugging\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "from unsloth import FastModel,FastLanguageModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from openai_moderation_dataset import build_training_dataloader, build_evaluation_dataloader\n",
    "from peft import PeftModel\n",
    "from openai_moderation_rules import DATA3_MODERATION_COLUMN_VARIANTS, OPENAI_MODERATION_COLUMN_VARIANTS\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    # model_name = \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-4B-Base-unsloth-bnb-4bit\",\n",
    "    model_name = \"unsloth/Qwen3-1.7B-Base-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    # model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    # model_name=\"unsloth/gemma-3-12b-pt\",\n",
    "    # model_name=\"unsloth/gemma-3-4b-pt\",\n",
    "    # max_seq_length = 8192, # Choose any for long context!\n",
    "    # resize_model_vocab = 80999, \n",
    "    load_in_4bit = True,\n",
    ")\n",
    "# model.model.embed_tokens.load_state_dict({'weight':torch.load('Model/reduced_embedding.pt')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb672bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_logits = True\n",
    "temperature = 1.0\n",
    "sample_temperature = 0.5\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185ce874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2308, 7414]\n"
     ]
    }
   ],
   "source": [
    "lm_head = nn.Linear(model.lm_head.in_features, 1, bias=True)\n",
    "# save the lm_head for yes and no\n",
    "index = [tokenizer.encode(\" No\")[0], tokenizer.encode(\" Yes\")[0]]\n",
    "print(index)\n",
    "lm_head.weight.data = (model.lm_head.weight[index[1]] - model.lm_head.weight[index[0]]).unsqueeze(0)\n",
    "nn.init.constant_(lm_head.bias, -8.0)\n",
    "lm_head = lm_head.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797674d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score columns:\n",
      "harassment_score\n",
      "harassment_threatening_score\n",
      "hate_score\n",
      "hate_threatening_score\n",
      "self_harm_score\n",
      "self_harm_instructions_score\n",
      "self_harm_intent_score\n",
      "sexual_score\n",
      "sexual_minors_score\n",
      "violence_score\n",
      "train score columns:\n",
      "severe_toxicity\n",
      "obscene\n",
      "threat\n",
      "insult\n",
      "identity_attack\n",
      "sexual_explicit\n",
      "You are a moderator. Check if the comment violates the rule.\n",
      "Rule: Be kind, not cruel — skip the harassment\n",
      "User comment: >serbia being aggressive is what directly caused wwi.austria-hungary annexed bosnia asked shit ton of demands from serbia which would cancel our teritorrial integrity. what the fuck are you talking about? .\\> genocide .yall throwing this word around and you don't even know what genocide means\n",
      "Does it violate?\n",
      "Does the comment violate the reddit rule?\n",
      "Rule: Be respectful — no harassment or abuse\n",
      "Comment: >that means what exactly?.that someone didn't read the study nor understands the science nor wants to understand the science nor appreciates the insane amount of work that goes into getting a {name} and just wants to talk scepticism about something they probably know nothing about\n",
      "Violation:\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "tensor([-1.9196, -3.8349])\n",
      "['harassment_score', 'harassment_score']\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = build_training_dataloader(\"Data/data_full/balanced/data-en/train_optimized_moderation_normalized.csv\", \\\n",
    "                                             tokenizer, OPENAI_MODERATION_COLUMN_VARIANTS,\\\n",
    "                                             sample_uniform_mix=0.01, return_logits=use_logits, temperature=temperature, sample_temperature=sample_temperature)\n",
    "eval_dataloader = build_evaluation_dataloader(\"Data/Data3/data3_sample_50k_len_sorted.csv\", tokenizer, DATA3_MODERATION_COLUMN_VARIANTS,\\\n",
    "                                              text_name=\"comment_text\", batch_size=8)\n",
    "for i,(input_ids, labels, length, metadata) in enumerate(train_dataloader):\n",
    "    print(tokenizer.decode(input_ids[0]))\n",
    "    print(tokenizer.decode(input_ids[1]))\n",
    "    print(labels)\n",
    "    print(metadata)\n",
    "    if i > -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142f611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de811703",
   "metadata": {},
   "source": [
    "#### Fine-tune lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc07711",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = int(64 / batch_size)\n",
    "lr = 1e-4\n",
    "clip = 1.0\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f133e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [param for param in lm_head.parameters() if param.requires_grad]\n",
    "# trainable_params = [param for param in model.model.parameters() if param.requires_grad] + [lm_head_weight]\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': [lm_head_weight], 'lr': lr},\n",
    "#     {'params': [param for param in model.model.parameters() if param.requires_grad], 'lr': lr * 0.2}])\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=lr)\n",
    "loss_fn = torch.nn.HuberLoss(delta=2.0) if use_logits else torch.nn.BCEWithLogitsLoss()\n",
    "print(f\"Trainable parameters: {len(trainable_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3344bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 4000/508193 | Loss: 7.5750\n",
      "Epoch: 0 | Step: 8000/508193 | Loss: 7.2550\n",
      "Epoch: 0 | Step: 12000/508193 | Loss: 6.9571\n",
      "Epoch: 0 | Step: 16000/508193 | Loss: 6.8773\n",
      "Epoch: 0 | Step: 20000/508193 | Loss: 6.5562\n",
      "Epoch: 0 | Step: 24000/508193 | Loss: 6.3760\n",
      "Epoch: 0 | Step: 28000/508193 | Loss: 6.2238\n",
      "Epoch: 0 | Step: 32000/508193 | Loss: 6.2716\n",
      "Epoch: 0 | Step: 36000/508193 | Loss: 6.1605\n",
      "Epoch: 0 | Step: 40000/508193 | Loss: 6.1337\n",
      "--- End of Epoch 0 --- Average Loss: 6.6384 ---\n",
      "Total training time: 14.10 minutes\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "logging_steps = 4000\n",
    "# --- Training Loop ---\n",
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "# New accumulator for more frequent logging\n",
    "logging_loss_accum = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode    \n",
    "    for i, (input_ids, labels, length, metadata) in enumerate(train_dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, length, labels = input_ids.to('cuda'), length.to('cuda'), labels.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                output = model.model(input_ids)\n",
    "            logits = lm_head(output.last_hidden_state[torch.arange(input_ids.shape[0]), length])\n",
    "            loss = loss_fn(logits.squeeze(1), labels)\n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = loss / accumulation_steps\n",
    "        \n",
    "        # Accumulate loss values for logging\n",
    "        train_loss_accum += loss.item()\n",
    "        logging_loss_accum += loss.item()\n",
    "        scaled_loss.backward()\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_norm_(trainable_params, clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # --- New Logging Block ---\n",
    "        # Print loss every `logging_steps`\n",
    "        if (i + 1) % logging_steps == 0:\n",
    "            # Calculate the average loss for the last `logging_steps`\n",
    "            avg_logging_loss = logging_loss_accum / logging_steps\n",
    "            print(f\"Epoch: {epoch} | Step: {i+1}/{len(train_dataloader)} | Loss: {avg_logging_loss:.4f}\")\n",
    "            # Reset the logging accumulator\n",
    "            logging_loss_accum = 0\n",
    "        \n",
    "        if i > logging_steps * 10:\n",
    "            break\n",
    "    # --- End of Epoch Summary ---\n",
    "    # Calculate the average loss for the entire epoch\n",
    "    epoch_avg_loss = train_loss_accum / (i + 1)\n",
    "    print(f\"--- End of Epoch {epoch} --- Average Loss: {epoch_avg_loss:.4f} ---\")\n",
    "    # Reset accumulator for the next epoch\n",
    "    train_loss_accum = 0\n",
    "\n",
    "print(f\"Total training time: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fa74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e4abe7",
   "metadata": {},
   "source": [
    "#### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947689ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "accumulation_steps = 64\n",
    "lr = 2e-5\n",
    "clip = 1.0\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4aaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "394\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "FastLanguageModel.for_training(model)\n",
    "trainable_params = [param for param in model.parameters() if param.requires_grad] + [param for param in lm_head.parameters() if param.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr = lr)\n",
    "print(len(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da030440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 4000/508193 | Loss: 4.3232\n",
      "Epoch: 0 | Step: 8000/508193 | Loss: 2.8221\n",
      "Epoch: 0 | Step: 12000/508193 | Loss: 2.3124\n",
      "Epoch: 0 | Step: 16000/508193 | Loss: 2.0111\n",
      "Epoch: 0 | Step: 20000/508193 | Loss: 1.7953\n",
      "Epoch: 0 | Step: 24000/508193 | Loss: 1.6751\n",
      "Epoch: 0 | Step: 28000/508193 | Loss: 1.6220\n",
      "Epoch: 0 | Step: 32000/508193 | Loss: 1.4796\n",
      "Epoch: 0 | Step: 36000/508193 | Loss: 1.4801\n",
      "Epoch: 0 | Step: 40000/508193 | Loss: 1.4189\n",
      "Epoch: 0 | Step: 44000/508193 | Loss: 1.3746\n",
      "Epoch: 0 | Step: 48000/508193 | Loss: 1.3933\n",
      "Epoch: 0 | Step: 52000/508193 | Loss: 1.3224\n",
      "Epoch: 0 | Step: 56000/508193 | Loss: 1.3212\n",
      "Epoch: 0 | Step: 60000/508193 | Loss: 1.2789\n",
      "Epoch: 0 | Step: 64000/508193 | Loss: 1.2706\n",
      "Epoch: 0 | Step: 68000/508193 | Loss: 1.2452\n",
      "Epoch: 0 | Step: 72000/508193 | Loss: 1.3299\n",
      "Epoch: 0 | Step: 76000/508193 | Loss: 1.2533\n",
      "Epoch: 0 | Step: 80000/508193 | Loss: 1.2234\n",
      "Epoch: 0 | Step: 84000/508193 | Loss: 1.1881\n",
      "Epoch: 0 | Step: 88000/508193 | Loss: 1.1861\n",
      "Epoch: 0 | Step: 92000/508193 | Loss: 1.2093\n",
      "Epoch: 0 | Step: 96000/508193 | Loss: 1.1654\n",
      "Epoch: 0 | Step: 100000/508193 | Loss: 1.1782\n",
      "Epoch: 0 | Step: 104000/508193 | Loss: 1.1459\n",
      "Epoch: 0 | Step: 108000/508193 | Loss: 1.1407\n",
      "Epoch: 0 | Step: 112000/508193 | Loss: 1.0994\n",
      "Epoch: 0 | Step: 116000/508193 | Loss: 1.1616\n",
      "Epoch: 0 | Step: 120000/508193 | Loss: 1.0764\n",
      "Epoch: 0 | Step: 124000/508193 | Loss: 1.1401\n",
      "Epoch: 0 | Step: 128000/508193 | Loss: 1.0699\n",
      "Epoch: 0 | Step: 132000/508193 | Loss: 1.0689\n",
      "Epoch: 0 | Step: 136000/508193 | Loss: 1.0650\n",
      "Epoch: 0 | Step: 140000/508193 | Loss: 1.0681\n",
      "Epoch: 0 | Step: 144000/508193 | Loss: 1.0742\n",
      "Epoch: 0 | Step: 148000/508193 | Loss: 1.0948\n",
      "Epoch: 0 | Step: 152000/508193 | Loss: 1.0675\n",
      "Epoch: 0 | Step: 156000/508193 | Loss: 1.0450\n",
      "Epoch: 0 | Step: 160000/508193 | Loss: 1.0616\n",
      "Epoch: 0 | Step: 164000/508193 | Loss: 1.0530\n",
      "Epoch: 0 | Step: 168000/508193 | Loss: 1.0561\n",
      "Epoch: 0 | Step: 172000/508193 | Loss: 1.0176\n",
      "Epoch: 0 | Step: 176000/508193 | Loss: 0.9879\n",
      "Epoch: 0 | Step: 180000/508193 | Loss: 0.9928\n",
      "Epoch: 0 | Step: 184000/508193 | Loss: 1.0090\n",
      "Epoch: 0 | Step: 188000/508193 | Loss: 1.0021\n",
      "Epoch: 0 | Step: 192000/508193 | Loss: 1.0268\n",
      "Epoch: 0 | Step: 196000/508193 | Loss: 0.9857\n",
      "Epoch: 0 | Step: 200000/508193 | Loss: 0.9765\n",
      "Epoch: 0 | Step: 204000/508193 | Loss: 1.0427\n",
      "Epoch: 0 | Step: 208000/508193 | Loss: 0.9706\n",
      "Epoch: 0 | Step: 212000/508193 | Loss: 0.9966\n",
      "Epoch: 0 | Step: 216000/508193 | Loss: 0.9439\n",
      "Epoch: 0 | Step: 220000/508193 | Loss: 0.9756\n",
      "Epoch: 0 | Step: 224000/508193 | Loss: 0.9657\n",
      "Epoch: 0 | Step: 228000/508193 | Loss: 0.9349\n",
      "Epoch: 0 | Step: 232000/508193 | Loss: 0.9282\n",
      "Epoch: 0 | Step: 236000/508193 | Loss: 0.9523\n",
      "Epoch: 0 | Step: 240000/508193 | Loss: 0.9225\n",
      "Epoch: 0 | Step: 244000/508193 | Loss: 0.9454\n",
      "Epoch: 0 | Step: 248000/508193 | Loss: 0.9530\n",
      "Epoch: 0 | Step: 252000/508193 | Loss: 0.9547\n",
      "Epoch: 0 | Step: 256000/508193 | Loss: 0.9866\n",
      "Epoch: 0 | Step: 260000/508193 | Loss: 0.9530\n",
      "Epoch: 0 | Step: 264000/508193 | Loss: 0.9318\n",
      "Epoch: 0 | Step: 268000/508193 | Loss: 0.9577\n",
      "Epoch: 0 | Step: 272000/508193 | Loss: 0.9543\n",
      "Epoch: 0 | Step: 276000/508193 | Loss: 0.9307\n",
      "Epoch: 0 | Step: 280000/508193 | Loss: 0.9429\n",
      "Epoch: 0 | Step: 284000/508193 | Loss: 0.9110\n",
      "Epoch: 0 | Step: 288000/508193 | Loss: 0.9621\n",
      "Epoch: 0 | Step: 292000/508193 | Loss: 0.9410\n",
      "Epoch: 0 | Step: 296000/508193 | Loss: 0.9233\n",
      "Epoch: 0 | Step: 300000/508193 | Loss: 0.9643\n",
      "Epoch: 0 | Step: 304000/508193 | Loss: 0.9162\n",
      "Epoch: 0 | Step: 308000/508193 | Loss: 0.9277\n",
      "Epoch: 0 | Step: 312000/508193 | Loss: 0.9191\n",
      "Epoch: 0 | Step: 316000/508193 | Loss: 0.9165\n",
      "Epoch: 0 | Step: 320000/508193 | Loss: 0.8904\n",
      "Epoch: 0 | Step: 324000/508193 | Loss: 0.8794\n",
      "Epoch: 0 | Step: 328000/508193 | Loss: 0.9130\n",
      "Epoch: 0 | Step: 332000/508193 | Loss: 0.8920\n",
      "Epoch: 0 | Step: 336000/508193 | Loss: 0.9059\n",
      "Epoch: 0 | Step: 340000/508193 | Loss: 0.9055\n",
      "Epoch: 0 | Step: 344000/508193 | Loss: 0.9150\n",
      "Epoch: 0 | Step: 348000/508193 | Loss: 0.8642\n",
      "Epoch: 0 | Step: 352000/508193 | Loss: 0.8873\n",
      "Epoch: 0 | Step: 356000/508193 | Loss: 0.8717\n",
      "Epoch: 0 | Step: 360000/508193 | Loss: 0.8925\n",
      "Epoch: 0 | Step: 364000/508193 | Loss: 0.8703\n",
      "Epoch: 0 | Step: 368000/508193 | Loss: 0.8690\n",
      "Epoch: 0 | Step: 372000/508193 | Loss: 0.8493\n",
      "Epoch: 0 | Step: 376000/508193 | Loss: 0.8675\n",
      "Epoch: 0 | Step: 380000/508193 | Loss: 0.8565\n",
      "Epoch: 0 | Step: 384000/508193 | Loss: 0.8939\n",
      "Epoch: 0 | Step: 388000/508193 | Loss: 0.8795\n",
      "Epoch: 0 | Step: 392000/508193 | Loss: 0.8576\n",
      "Epoch: 0 | Step: 396000/508193 | Loss: 0.8621\n",
      "Epoch: 0 | Step: 400000/508193 | Loss: 0.8819\n",
      "Epoch: 0 | Step: 404000/508193 | Loss: 0.8618\n",
      "Epoch: 0 | Step: 408000/508193 | Loss: 0.8705\n",
      "Epoch: 0 | Step: 412000/508193 | Loss: 0.8722\n",
      "Epoch: 0 | Step: 416000/508193 | Loss: 0.8838\n",
      "Epoch: 0 | Step: 420000/508193 | Loss: 0.8946\n",
      "Epoch: 0 | Step: 424000/508193 | Loss: 0.8615\n",
      "Epoch: 0 | Step: 428000/508193 | Loss: 0.8469\n",
      "Epoch: 0 | Step: 432000/508193 | Loss: 0.8350\n",
      "Epoch: 0 | Step: 436000/508193 | Loss: 0.8493\n",
      "Epoch: 0 | Step: 440000/508193 | Loss: 0.8583\n",
      "Epoch: 0 | Step: 444000/508193 | Loss: 0.8927\n",
      "Epoch: 0 | Step: 448000/508193 | Loss: 0.9174\n",
      "Epoch: 0 | Step: 452000/508193 | Loss: 0.8398\n",
      "Epoch: 0 | Step: 456000/508193 | Loss: 0.8564\n",
      "Epoch: 0 | Step: 460000/508193 | Loss: 0.8637\n",
      "Epoch: 0 | Step: 464000/508193 | Loss: 0.8491\n",
      "Epoch: 0 | Step: 468000/508193 | Loss: 0.9195\n",
      "Epoch: 0 | Step: 472000/508193 | Loss: 0.8370\n",
      "Epoch: 0 | Step: 476000/508193 | Loss: 0.8421\n",
      "Epoch: 0 | Step: 480000/508193 | Loss: 0.8599\n",
      "Epoch: 0 | Step: 484000/508193 | Loss: 0.8204\n",
      "Epoch: 0 | Step: 488000/508193 | Loss: 0.8251\n",
      "Epoch: 0 | Step: 492000/508193 | Loss: 0.8404\n",
      "Epoch: 0 | Step: 496000/508193 | Loss: 0.8260\n",
      "Epoch: 0 | Step: 500000/508193 | Loss: 0.8213\n",
      "Epoch: 0 | Step: 504000/508193 | Loss: 0.8275\n",
      "Epoch: 0 | Step: 508000/508193 | Loss: 0.8441\n",
      "--- End of Epoch 0 --- Average Loss: 1.0670 ---\n",
      "Total training time: 728.24 minutes\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "logging_steps = 4000\n",
    "# --- Training Loop ---\n",
    "start_time = time.time()\n",
    "train_loss_accum = 0\n",
    "# New accumulator for more frequent logging\n",
    "logging_loss_accum = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set model to training mode    \n",
    "    for i, (input_ids, labels, length, metadata) in enumerate(train_dataloader):\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            input_ids, length, labels = input_ids.to('cuda'), length.to('cuda'), labels.to('cuda')\n",
    "            output = model.base_model.model.model(input_ids)\n",
    "            logits = lm_head(output.last_hidden_state[torch.arange(input_ids.shape[0]), length])\n",
    "            loss = loss_fn(logits.squeeze(1), labels)\n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = loss / accumulation_steps\n",
    "        \n",
    "        # Accumulate loss values for logging\n",
    "        train_loss_accum += loss.item()\n",
    "        logging_loss_accum += loss.item()\n",
    "        scaled_loss.backward()\n",
    "        \n",
    "        # Gradient accumulation step\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_norm_(trainable_params, clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # --- New Logging Block ---\n",
    "        # Print loss every `logging_steps`\n",
    "        if (i + 1) % logging_steps == 0:\n",
    "            # Calculate the average loss for the last `logging_steps`\n",
    "            avg_logging_loss = logging_loss_accum / logging_steps\n",
    "            print(f\"Epoch: {epoch} | Step: {i+1}/{len(train_dataloader)} | Loss: {avg_logging_loss:.4f}\")\n",
    "            # Reset the logging accumulator\n",
    "            logging_loss_accum = 0\n",
    "\n",
    "    # --- End of Epoch Summary ---\n",
    "    # Calculate the average loss for the entire epoch\n",
    "    epoch_avg_loss = train_loss_accum / len(train_dataloader)\n",
    "    print(f\"--- End of Epoch {epoch} --- Average Loss: {epoch_avg_loss:.4f} ---\")\n",
    "    # Reset accumulator for the next epoch\n",
    "    train_loss_accum = 0\n",
    "\n",
    "print(f\"Total training time: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfd7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/zhenlan/Desktop/Projects/AI_Moderation/Model/merged_model4b{}\".format(save_path_surfix))\n",
    "torch.save(lm_head.state_dict(), \"/home/zhenlan/Desktop/Projects/AI_Moderation/Model/lm_head_weight{}.pth\".format(save_path_surfix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf4503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f49f3d24",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# model = PeftModel.from_pretrained(model, \"/home/zhenlan/Desktop/Projects/AI_Moderation/Model/merged_model4b{}\".format(save_path_surfix), is_trainable=False)\n",
    "# lm_head = nn.Linear(model.lm_head.in_features, 1, bias=True)\n",
    "# lm_head.load_state_dict(torch.load(\"/home/zhenlan/Desktop/Projects/AI_Moderation/Model/lm_head_weight{}.pth\".format(save_path_surfix)))\n",
    "# FastLanguageModel.for_inference(model)\n",
    "# model.eval()\n",
    "# lm_head.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cf8de",
   "metadata": {},
   "source": [
    "Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score columns:\n",
      "harassment_score\n",
      "harassment_threatening_score\n",
      "hate_score\n",
      "hate_threatening_score\n",
      "self_harm_score\n",
      "self_harm_instructions_score\n",
      "self_harm_intent_score\n",
      "sexual_score\n",
      "sexual_minors_score\n",
      "violence_score\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = build_training_dataloader(\"Data/data_full/balanced/data-en/val_optimized_moderation.csv\", \\\n",
    "                                             tokenizer, OPENAI_MODERATION_COLUMN_VARIANTS,\\\n",
    "                                             batch_size=8,\n",
    "                                             sample_uniform_mix=0.01, return_logits=use_logits, temperature=temperature, sample_temperature=sample_temperature)\n",
    "results = defaultdict(list)\n",
    "for i, (input_ids, labels, length, metadata) in enumerate(train_dataloader):\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16), torch.no_grad():\n",
    "        input_ids, length  = input_ids.to('cuda'), length.to('cuda')\n",
    "        output = model.base_model.model.model(input_ids)\n",
    "        yhats = lm_head(output.last_hidden_state[torch.arange(input_ids.shape[0]), length])\n",
    "        for m,y,yhat in zip(metadata, labels, yhats):\n",
    "            results[m].append([y.item(), yhat[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9e5ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harassment_score: 0.9147\n",
      "hate_threatening_score: 0.3970\n",
      "self_harm_score: 0.8163\n",
      "self_harm_intent_score: 0.8004\n",
      "self_harm_instructions_score: 0.6421\n",
      "sexual_minors_score: 0.7006\n",
      "hate_score: 0.8408\n",
      "harassment_threatening_score: 0.7571\n",
      "sexual_score: 0.8988\n",
      "violence_score: 0.9375\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "for category, ys in results.items():\n",
    "    ys = np.array(ys)\n",
    "    auc = stats.spearmanr(ys[:, 0], ys[:, 1]).statistic\n",
    "    print(f\"{category}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae274a6a",
   "metadata": {},
   "source": [
    "Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c38fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "for i, (input_ids, labels, length, metadata) in enumerate(eval_dataloader):\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16), torch.no_grad():\n",
    "        input_ids, length  = input_ids.to('cuda'), length.to('cuda')\n",
    "        output = model.base_model.model.model(input_ids)\n",
    "        yhat = lm_head(output.last_hidden_state[torch.arange(input_ids.shape[0]), length])\n",
    "        for m,y,yhat in zip(metadata, labels, yhat):\n",
    "            results[m].append([y.item(), yhat[0].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82264504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_toxicity: 0.2863\n",
      "obscene: 0.3164\n",
      "threat: 0.2331\n",
      "insult: 0.5126\n",
      "identity_attack: 0.3918\n",
      "sexual_explicit: 0.2066\n"
     ]
    }
   ],
   "source": [
    "# batch size 8, disable random prompt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "for category, ys in results.items():\n",
    "    ys = np.array(ys)\n",
    "    auc = stats.spearmanr(ys[:, 0], ys[:, 1]).statistic\n",
    "    print(f\"{category}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efaa99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
